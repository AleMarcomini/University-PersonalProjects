{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v4bkwQoHr9i3"
      },
      "source": [
        "# Laboratory 1: setting up Twitter API"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21NaYdhRr9i7"
      },
      "source": [
        "## Set up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gNFOaZPIxyS2",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['TOKEN'] = #CENSORED"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GjUQcJpfrecr",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nwy6MeGur9i-"
      },
      "source": [
        "### Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ge5EaEur9jA",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "import requests \n",
        "import pandas as pd \n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IYIP9Ta4r9jR"
      },
      "source": [
        "### Set up headers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pKWoILn7AMzj",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "def create_headers(bearer_token):\n",
        "    headers = {\"Authorization\": \"Bearer {}\".format(bearer_token)}\n",
        "    return headers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W_buwaoAyPLM",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "headers = create_headers(os.environ['TOKEN'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q8Pb9MXWr9jd"
      },
      "source": [
        "## Download data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "90Ukr82M_N39",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "def create_url(keyword, start_date, end_date, env_label, endpoint=\"fullarchive\"):\n",
        "    \n",
        "    search_url = \"https://api.twitter.com/1.1/tweets/search/{}.json\".format(endpoint+\"/\"+env_label) \n",
        "\n",
        "    #change params based on the endpoint you are using\n",
        "    query_params = {'query': keyword, 'fromDate': start_date, 'toDate': end_date}\n",
        "    return (search_url, query_params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AJD9pEHwAk84",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "def connect_to_endpoint(url, headers, params, next_token = None):\n",
        "    if next_token is not None and next_token != '':\n",
        "      params['next'] = next_token\n",
        "    response = requests.request(\"GET\", url, headers = headers, params = params)\n",
        "    if response.status_code != 200:\n",
        "        raise Exception(response.status_code, response.text)\n",
        "    return response.json()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Jgw8uZd0K5u",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "def get_data(keyword, start_time, end_time, next_token, env_label, endpoint):\n",
        "  results = []\n",
        "  \n",
        "  while next_token is not None:\n",
        "    ##this part here for one request\n",
        "    url = create_url(keyword, start_time,end_time, env_label, endpoint)\n",
        "    json_response = connect_to_endpoint(url[0], headers, url[1], next_token)\n",
        "    \n",
        "    if \"results\" in json_response:\n",
        "      results.extend(json_response[\"results\"])\n",
        "    ### up until this point\n",
        "    if \"next\" in json_response:\n",
        "        next_token = json_response[\"next\"]\n",
        "    else:\n",
        "      next_token = None\n",
        "    time.sleep(1)\n",
        "  \n",
        "  return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IyXiwYZCl8H3",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "def get_single_response(keyword, start_time, end_time, env_label, endpoint):\n",
        "  results = []\n",
        "  url = create_url(keyword, start_time,end_time, env_label, endpoint)\n",
        "  json_response = connect_to_endpoint(url[0], headers, url[1])\n",
        "  \n",
        "  if \"results\" in json_response:\n",
        "    results.extend(json_response[\"results\"])\n",
        "  \n",
        "  return results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vcJ4mF6d09-y",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "tweets = get_data(\"Sarajevo\", \"202002090000\", \"202002100000\", \"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gSt4xdwfmHcd",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "tweets = get_single_response(\"COVID19 lang:en\", \"202110060000\", \"202111050000\", \"30day\", \"30day\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2tQoczST6dQ8"
      },
      "source": [
        "### Inspect data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qcWGG5Rz39Gr",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "len(tweets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qy1AZe4O4T7C",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "tweets[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nUGfn_G3qcLV"
      },
      "source": [
        "# Laboratory 2: working with Twitter data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-6mSYSqKqh7f"
      },
      "source": [
        "First, we want to convert the data into Pandas DataFrame. This format enables us easy manipulation of the data as well as saving/loading data.\n",
        "\n",
        "Since we have our tweets saved as a list of dictionaries, we can easily convert it to DataFrame by executing the cell blow."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u1C940uZnWFA",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "tweets_df = pd.DataFrame(tweets)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YVXP-fDrtMae"
      },
      "source": [
        "### Saving the results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kmi5SprXrEOM"
      },
      "source": [
        "Once we have our Tweets as a DataFrame it is a good idea to save it on the disk. \n",
        "\n",
        "Be mindful of the fact that the storage of a Colab notebook is deleted everytime runtime is interrupted or restarted, so you need to manually download it to your computer or mount your Google Drive and save it there (this option is unavailable if you're using university's email account for Drive)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PqdepSBprmvl",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "path = \"/content/\" #enter the path to your Drive or leave this as default"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CmtD5-THsUJp"
      },
      "source": [
        "We can save it as a comma-separated values file, which enables opening it in a spreadsheet editor and inspecting it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ex1E8v-qrkeh",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "tweets_df.to_csv(path+\"tweets.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iFcy4tKls7qD"
      },
      "source": [
        "In order to preserve datatypes, we should save it as a parquet or pickle file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hUqdVEGksm7n",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "tweets_df.to_pickle(path+\"tweets.pkl\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H02ZoRcGtpaY"
      },
      "source": [
        "### Loading the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VWhc-_nNtsnT"
      },
      "source": [
        "If you want to load the results you have previously saved, simply execute the next code, specifying the path to the file.\n",
        "\n",
        "You will need to either upload it to the Colab workspace or copy the path to the file on Drive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1LlenT11t_Xp",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "tweets_df = pd.read_pickle(path+\"tweets.pkl\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k9ItAgz7uga9"
      },
      "source": [
        "### Preprocessing the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WSvM1_xzu9Uc"
      },
      "source": [
        "In our dataframe we have the entire Tweet object. Some columns that might be of particular interest to us are: \n",
        "\n",
        "*   created_at - date when Tweet was posted\n",
        "*   id/id_str - unique Tweet identifiers\n",
        "*   text - the content of the Tweet\n",
        "*   user - information about the user who posted the Tweet\n",
        "*   retweeted_status  - information about the original Tweet\n",
        "*   quote/reply/retweet/favorite count - Tweet metrics\n",
        "*   entities - hashtags, urls, user_mentions present in Tweet\n",
        "\n",
        "We can filter the dataframe and keep only columns we are interested in. You can pick which columns you'd like to keep and put them int the column_list below.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AZYiG40aumbk",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "tweets_filtered = tweets_df.copy() #it's a good idea to work on the copy of original dataframe, so we can always go back to it if we mess something up\n",
        "column_list = [\"created_at\", \"id_str\", \"text\", \"user\", \"retweeted_status\", \"quote_count\", \"reply_count\", \"retweet_count\", \"favorite_count\", \"entities\"]\n",
        "tweets_filtered = tweets_filtered[column_list]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qQ90m-FpxM9N",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "tweets_filtered"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hPgzBUJj0SZU"
      },
      "source": [
        "## Extracting words/hashtags"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CygaLHAS0nzP"
      },
      "source": [
        "There are many ways to build networks from the data we download from Twitter.\n",
        "\n",
        "One possibility is to have a bipartite network of Tweets and words/hashtags and then observe word, hashtag or word-hashtag projections."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H1nTCbRc0-__"
      },
      "source": [
        "### Extracting words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8pGnokgK1jma"
      },
      "source": [
        "In order to extract words, we first need to clean the Tweet text. This way we will remove punctuation, hashtags/mentions/urls (they are preserved in the entity column anyway). We will also turn all letters to lowercase.\n",
        "\n",
        "You can also consider removing stopwords, removing words that are not in the english language corpora, lematizing the words, etc. I suggest you research nltk library and its possibilities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y5746Mq918dG",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import emoji\n",
        "import string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z8Nrv5jv1e5W",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "def cleaner(tweet):\n",
        "    tweet = re.sub(\"@[A-Za-z0-9]+\",\"\",tweet) # remove mentions\n",
        "    tweet = re.sub(\"#[A-Za-z0-9]+\", \"\",tweet) # remove hashtags\n",
        "    tweet = re.sub(r\"(?:\\@|http?\\://|https?\\://|www)\\S+\", \"\", tweet) # remove http links\n",
        "    tweet = \" \".join(tweet.split())\n",
        "    tweet = str.lower(tweet) #to lowercase\n",
        "    table = str.maketrans(dict.fromkeys(string.punctuation)) \n",
        "    tweet = tweet.translate(table)# remove punctuation         \n",
        "    return tweet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fEhs-tyy2naZ",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "tweets_filtered[\"clean_text\"] = tweets_filtered[\"text\"].map(cleaner)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I1_lSQZ85rT_"
      },
      "source": [
        "We are going to loop through the dataframe and then through the words in the clean text. We are going to add the words as keys to dictionary and use their frequencies as values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xiBKlJhV3d_S",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "#initialize an empty dict\n",
        "unique_words = {}\n",
        "for row in tweets_filtered.clean_text:\n",
        "  for word in row.split(\" \"):\n",
        "    #if the word is encountered for the first time add to dict as key and set its value to 0\n",
        "    unique_words.setdefault(word,0)\n",
        "    #increase the value (i.e the count) of the word by 1 every time it is encountered\n",
        "    unique_words[word] += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8k7riE1z7F6r",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "#remove blank space\n",
        "unique_words.pop(\" \")\n",
        "#remove word 'rt'\n",
        "unique_words.pop(\"rt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kVYvDBQu57If"
      },
      "source": [
        "We can inspect the words as a dataframe. \n",
        "\n",
        "\n",
        "You can always save this dataframe as .csv for future reference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_xI5aGQM53u4",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "uw_df = pd.DataFrame.from_dict(unique_words, orient='index').reset_index()\n",
        "uw_df.rename(columns = {'index':'Word', 0:'Count'}, inplace=True)\n",
        "uw_df.sort_values(by=['Count'], ascending=False, inplace=True)\n",
        "uw_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C4R-j-FN7Rgo"
      },
      "source": [
        "### Extracting the hashtags"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O8SfJwmf9GU2"
      },
      "source": [
        "We are going to loop through the dataframe and then through the hashtags in the entities. We are going to add the hashtags as keys to dictionary and use their frequencies as values. At the same time, we are going to save them in a list and add them to a separate column to facilitate our future work."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hHyc_WrGt5Tj",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "unique_hashtags = {}\n",
        "\n",
        "tweets_filtered[\"hashtags\"] = \"\"\n",
        "\n",
        "for idx, row in tweets_filtered.iterrows():\n",
        "  hashtag_list = []\n",
        "  for hashtag in row[\"entities\"][\"hashtags\"]:\n",
        "    unique_hashtags.setdefault(\"#\"+hashtag[\"text\"], 0)\n",
        "    unique_hashtags['#'+hashtag[\"text\"]] += 1\n",
        "    hashtag_list.append(hashtag[\"text\"])\n",
        "  tweets_filtered.at[idx,\"hashtags\"] = hashtag_list\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uCb7arJqUXOu",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "uh_df = pd.DataFrame.from_dict(unique_hashtags, orient='index').reset_index()\n",
        "uh_df.rename(columns = {'index':'Hashtag', 0:'Count'}, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "31Ydx_RbWSGc",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "uh_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wPaVj1tj9Uw6"
      },
      "source": [
        "## Building the network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vt2co2ep9YCd"
      },
      "source": [
        "We are going to use the networkx library, which is a Python library that enables network science analysis of the data.\n",
        "\n",
        "We are going to use it to create our network and extract edgelist from it, since we can easily import it to Gephi (a software we are going to see in visualization labs).\n",
        "\n",
        "However, it offers implemented algorithms for analysis (for example PageRank) that you can use out-of-box to analyze your network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gnd62ng6-GLW"
      },
      "source": [
        "But first, we will loop through our dataframe and connect words and hashtags if they appear together in the same Tweet."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BooMyc6-1JWa",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "import itertools\n",
        "import networkx as nx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "adLbCz86M7SR",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "uh = unique_hashtags.keys()\n",
        "uw = unique_words.keys()  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UHuQ3rRXOA5_",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "network = {}\n",
        "network_key = 0\n",
        "\n",
        "for index, row in tweets_filtered.iterrows():\n",
        "    #hashtags extracted from Tweet do not have the # sign in front of them but we will add it to differentiate hashtags from words\n",
        "    combined_list = ['#'+hashtag for hashtag in row[\"hashtags\"] if '#'+hashtag in unique_hashtags] + [word for word in str.split(row[\"clean_text\"], \" \") if word in uw]\n",
        "    #itertool product creates Cartesian product of each element in the combined list\n",
        "    for pair in itertools.product(combined_list, combined_list):\n",
        "        #exclude self-loops and count each pair only once because our graph is undirected and we do not take self-loops into account\n",
        "        if pair[0]!=pair[1] and not(pair[::-1] in network):\n",
        "            network.setdefault(pair,0)\n",
        "            network[pair] += 1 #* row[\"retweetCount\"]\n",
        "    \n",
        "network_df = pd.DataFrame.from_dict(network, orient=\"index\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8uThrYGHSdEe",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "network_df.reset_index(inplace=True)\n",
        "network_df.columns = [\"pair\",\"weight\"]\n",
        "network_df.sort_values(by=\"weight\",inplace=True, ascending=False)\n",
        "network_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NJvNvzGXy8Kg",
        "scrolled": false,
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "#to get weighted graph we need a list of 3-element tuplels (u,v,w) where u and v are nodes and w is a number representing weight\n",
        "up_weighted = []\n",
        "for edge in network:\n",
        "    #we can filter edges by weight by uncommenting the next line and setting desired weight threshold\n",
        "    #if(network[edge])>1:\n",
        "    up_weighted.append((edge[0],edge[1],network[edge]))\n",
        "\n",
        "G = nx.Graph()\n",
        "G.add_weighted_edges_from(up_weighted)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eSneLIqZNvt1",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "print(len(G.nodes()))\n",
        "print(len(G.edges()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mj3CwR5Cy8Kk"
      },
      "source": [
        "#### SAVE EDGELIST"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fFtpm869ONHg",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "filename = \"./edgelist.csv\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PTmGSBc3y8Kn",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "nx.write_weighted_edgelist(G, filename, delimiter=\",\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GlboURoYy8Kp",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "#add header with appropriate column names (works on collab and Linux/Mac(?))\n",
        "!sed -i.bak 1i\"Source,Target,Weight\" ./edgelist.csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e5oT2lSry8Kq"
      },
      "source": [
        "### Create Node List\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lpef5RKvUu_w",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "word_nodes = pd.DataFrame.from_dict(unique_words,orient=\"index\")\n",
        "word_nodes.reset_index(inplace=True)\n",
        "word_nodes[\"Label\"] = word_nodes[\"index\"]\n",
        "word_nodes.rename(columns={\"index\":\"Id\",0:\"delete\"},inplace=True)\n",
        "word_nodes = word_nodes.drop(columns=['delete'])\n",
        "\n",
        "word_nodes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZMdIcS4my8Ks",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "hashtag_nodes = uh_df.copy()\n",
        "hashtag_nodes[\"Label\"] = hashtag_nodes[\"Hashtag\"]\n",
        "hashtag_nodes.rename(columns={\"Hashtag\":\"Id\"},inplace=True)\n",
        "hashtag_nodes = hashtag_nodes.drop(columns=['Count'])\n",
        "hashtag_nodes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BHZHIG18ye0F"
      },
      "source": [
        "#### SAVE NODELIST"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PpE0P0cIU2OD",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "nodelist = hashtag_nodes.append(word_nodes, ignore_index=True)\n",
        "\n",
        "nodelist.to_csv(\"nodelist.csv\",index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ryR3LiMVB-70"
      },
      "source": [
        "Tasks: \n",
        "\n",
        "*   Extract username of user who posted the tweet into a column \"screen_name\". Follow the procedure we used to get the hashtags.\n",
        "*   Create a network of users using the mention relation. Is this a directed or undirected graph?\n",
        "*   We created a network where nodes are mixed (both words and hashtags). Create network of words only and one of hashtags only.\n",
        "* Pick one of these network and rank the nodes using PageRank centrality. Extract information about top-20 rated nodes.\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Tweet API.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
